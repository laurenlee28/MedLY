import os
import gc
import torch
import torchaudio
from pipeline import MedicalVoicePipeline  # pipeline.pyì— ì „ì²´ í´ë˜ìŠ¤ ì •ì˜ ìˆë‹¤ê³  ê°€ì •

# Step 1: Input Audio File
audio_input_path = "sample_audio.mp3"

if not os.path.exists(audio_input_path):
    print(f"ğŸ›‘ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_input_path}")
    exit(1)

# Step 2: Convert to compatible format
converted_path = "audio.wav"
try:
    waveform, sample_rate = torchaudio.load(audio_input_path)
    torchaudio.save(converted_path, waveform, sample_rate)
    print(f"âœ… ë³€í™˜ ì™„ë£Œ: '{audio_input_path}' â†’ '{converted_path}'")
except Exception as e:
    print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
    exit(1)

# Step 3: Define model configs
whisper_config = {'model_name': 'small'}
ner_config = {'model_weights_path': './ner_saved_model/ner_saved_model/model_weights.pth'}
qwen_config = {'model_path': 'Qwen/Qwen2-1.5B-Instruct'}

# Step 4: Run the pipeline
pipeline = MedicalVoicePipeline(whisper_config, ner_config, qwen_config)
result = pipeline.run(converted_path)

# Step 5: Output
if result:
    print("\nğŸ¯ ìµœì¢… ê²°ê³¼ ìš”ì•½:")
    print(f"ğŸ“ í…ìŠ¤íŠ¸: {result['text']}")
    print(f"ğŸ©º ìš©ì–´: {result['terms']}")
    print("ğŸ“˜ ì„¤ëª…:")
    for term, explanation in result['explanations'].items():
        print(f" - {term}: {explanation}")
else:
    print("ğŸ›‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")

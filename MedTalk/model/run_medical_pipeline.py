# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iNdFdBNvuCLs7jyTvCM6TTgs864UIWAd
"""

# ==============================================================================
#                 MEDICAL VOICE ASSISTANT - LOCAL ENVIRONMENT (v6)
#
# Instructions:
# 1. Ensure all dependencies are installed and MeCab is set up for your OS.
# 2. Make sure your folder structure is correct.
# 3. Modify the file paths at the bottom of the script if needed.
#
# ==============================================================================

# -*- coding: utf-8 -*-
import os
import sys
import torch
import torch.nn as nn
import gc
import whisper
from transformers import AutoTokenizer, pipeline, ElectraModel, AutoModelForCausalLM, TextGenerationPipeline
from konlpy.tag import Mecab
import subprocess
import json

print("ğŸ› ï¸ run_medical_pipeline.py ì‹¤í–‰ë¨!")

if len(sys.argv) > 1:
    mp3_path = sys.argv[1]
    print(f"ğŸ“¥ ì „ë‹¬ë°›ì€ mp3 ê²½ë¡œ: {mp3_path}")
else:
    print("â— mp3 ê²½ë¡œ ì¸ì ì—†ìŒ")
    sys.exit(1)

# === [1/4] Define Wrappers ===
print("ğŸš€ [1/4] Defining component wrappers...")

class WhisperWrapper:
    def __init__(self, model_name="small"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - Whisper using device: {self.device}")
        self.model = whisper.load_model(model_name, device=self.device)
    def transcribe(self, audio_path):
        if not os.path.exists(audio_path): return None
        return self.model.transcribe(audio_path, language="ko")["text"]

class ElectraForTokenClassification(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.electra = ElectraModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.electra.config.hidden_size, num_labels)
    def forward(self, input_ids, attention_mask=None):
        return {"logits": self.classifier(self.electra(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state)}

class MedicalNerWrapper:
    def __init__(self, model_weights_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - NER model using device: {self.device}")
        self.label_list = ["O", "B-Disease", "I-Disease", "B-Treatment", "I-Treatment", "B-Body", "I-Body"]
        self.id2label = {i: label for i, label in enumerate(self.label_list)}
        base_model_name = "SungJoo/medical-ner-koelectra"
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.model = ElectraForTokenClassification(base_model_name, len(self.label_list))
        self.model.load_state_dict(torch.load(model_weights_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        self.mecab = Mecab(dicpath="/opt/homebrew/lib/mecab/dic/mecab-ko-dic")
    def _reconstruct_terms(self, tokens, labels):
        terms, current_term = [], ""
        for token, label in zip(tokens, labels):
            if label.startswith("B-"):
                if current_term: terms.append(current_term)
                current_term = token
            elif label.startswith("I-"): current_term += token
            elif current_term:
                terms.append(current_term)
                current_term = ""
        if current_term: terms.append(current_term)
        return terms
    def extract_terms(self, text):
        tokens = self.mecab.morphs(text)
        tokenized_inputs = self.tokenizer(tokens, truncation=True, is_split_into_words=True, return_tensors="pt")
        inputs_for_model = {k: v.to(self.device) for k, v in tokenized_inputs.items() if k != "token_type_ids"}
        with torch.no_grad():
            logits = self.model(**inputs_for_model)["logits"]
        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()
        word_ids = tokenized_inputs.word_ids(batch_index=0)
        final_labels = ["O"] * len(tokens)
        for token_idx, word_idx in enumerate(word_ids):
            if word_idx is not None and predictions[token_idx] != -100:
                final_labels[word_idx] = self.id2label[predictions[token_idx]]
        return list(dict.fromkeys(self._reconstruct_terms(tokens, final_labels)))

class QwenWrapper:
    def __init__(self, model_path="Qwen/Qwen2.5-3B"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - QWEN model using device: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)
        self.generator = TextGenerationPipeline(model=self.model, tokenizer=self.tokenizer, device=0 if self.device == "cuda" else -1)

    def explain_terms(self, term_list):
        explanations = {}

        # ğŸ” ë””ë²„ê¹… ë¡œê·¸
        print(f"[ğŸ§ª DEBUG] ì…ë ¥ëœ term_list: {term_list}")
        print(f"[ğŸ§ª DEBUG] term_list íƒ€ì…: {type(term_list)}")

        if not term_list or not isinstance(term_list, list):
            print("â„¹ï¸ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜ëª»ëœ íƒ€ì…ì…ë‹ˆë‹¤. Qwen ì‹¤í–‰ ìƒëµ.")
            return explanations

        for term in term_list:
            prompt = f"ì˜í•™ ìš©ì–´ '{term}'ì— ëŒ€í•´ì„œ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•´ì¤˜."
            print(f"[ğŸ§ª DEBUG] prompt: {prompt}")
            try:
                output = self.generator(prompt, max_new_tokens=256)
                print(f"[ğŸ§ª DEBUG] output: {output}")
                explanations[term] = output[0]['generated_text']
            except Exception as e:
                print(f"[âŒ EXCEPTION] Qwen ìƒì„± ì‹¤íŒ¨: {e}")
                raise e

        return explanations



print("âœ… Component wrappers defined.")

# === [2/4] Define Main Pipeline ===
print("\nğŸš€ [2/4] Defining main pipeline with sequential loading...")

class MedicalVoicePipeline:
    def __init__(self, whisper_config, ner_config, qwen_config):
        self.whisper_config = whisper_config
        self.ner_config = ner_config
        self.qwen_config = qwen_config

    def run(self, audio_path):
        print("\n" + "="*50 + "\nğŸ”¬ PIPELINE RUNNING ğŸ”¬\n" + "="*50)

        print("\n[Step 1/3] Transcribing...")
        whisper_model = WhisperWrapper(**self.whisper_config)
        text = whisper_model.transcribe(audio_path)
        del whisper_model
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… Whisper done.")
        if not text:
            print("ğŸ›‘ Transcription failed.")
            return "", {}

        print(f"âœ… Transcription:\n{text}\n")

        print("[Step 2/3] NER extracting...")
        ner_model = MedicalNerWrapper(**self.ner_config)
        terms = ner_model.extract_terms(text)

        print(f"[DEBUG] ì¶”ì¶œëœ terms: {terms}")
        if not terms or not isinstance(terms, list):
            print("ğŸ›‘ NER ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ ì•„ë‹˜ â†’ Qwen ì‹¤í–‰ ìƒëµ")
            return text, {}

        del ner_model
        gc.collect()
        torch.cuda.empty_cache()
        print(f"âœ… Terms: {terms}")

        print("[Step 3/3] Explaining terms...")
        print(f"[DEBUG] Qwenì— ì „ë‹¬ë  terms: {terms}")
        print(f"[DEBUG] type: {type(terms)} / len: {len(terms)}")

        qwen_model = QwenWrapper(**self.qwen_config)

        
        explanations = qwen_model.explain_terms(terms)
        del qwen_model
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… Explanations done.")

        return text, explanations

print("âœ… Main pipeline defined.")

# === [3/4] Check model paths ===
print("\nğŸš€ [3/4] Checking for model files...")

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ner_path = os.path.join(BASE_DIR, "model", "ner_saved_model", "model_weights.pth")
qwen_path = os.path.join(BASE_DIR, "model", "qwen_model")

if not os.path.exists(ner_path):
    print(f"âŒ NER model not found at: {ner_path}")
    sys.exit(1)
if not os.path.exists(qwen_path):
    print(f"âŒ Qwen model not found at: {qwen_path}")
    sys.exit(1)

print("âœ… Model files found.")

# === [4/4] Convert MP3 and run pipeline ===
print("\nğŸš€ [4/4] Starting execution...")

def convert_mp3_to_wav(mp3_path, wav_path="converted_audio.wav"):
    if not os.path.exists(mp3_path):
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {mp3_path}")
    result = subprocess.run(["ffmpeg", "-y", "-i", mp3_path, wav_path],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        print(result.stderr.decode())
        raise RuntimeError("âŒ MP3 â†’ WAV ë³€í™˜ ì‹¤íŒ¨")
    print(f"âœ… MP3 â†’ WAV ë³€í™˜ ì™„ë£Œ: {wav_path}")
    return wav_path

try:
    wav_path = convert_mp3_to_wav(mp3_path)

    whisper_config = {'model_name': 'small'}
    ner_config = {'model_weights_path': ner_path}
    qwen_config = {'model_path': qwen_path}

    pipeline_instance = MedicalVoicePipeline(whisper_config, ner_config, qwen_config)
    text, explanations = pipeline_instance.run(wav_path)

    # ì €ì¥
    json_save_path = mp3_path.replace(".mp3", ".json")
    with open(json_save_path, "w", encoding="utf-8") as f:
        json.dump({"text": text, "explanations": explanations}, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_save_path}")

except Exception as e:
    print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iNdFdBNvuCLs7jyTvCM6TTgs864UIWAd
"""

# ==============================================================================
#                 MEDICAL VOICE ASSISTANT - LOCAL ENVIRONMENT (v6)
#
# Instructions:
# 1. Ensure all dependencies are installed and MeCab is set up for your OS.
# 2. Make sure your folder structure is correct.
# 3. Modify the file paths at the bottom of the script if needed.
#
# ==============================================================================

# -*- coding: utf-8 -*-
import os
import sys
import torch
import torch.nn as nn
import gc
import whisper
from transformers import AutoTokenizer, pipeline, ElectraModel, AutoModelForCausalLM, TextGenerationPipeline
from konlpy.tag import Mecab
import subprocess
import json

print("🛠️ run_medical_pipeline.py 실행됨!")

if len(sys.argv) > 1:
    mp3_path = sys.argv[1]
    print(f"📥 전달받은 mp3 경로: {mp3_path}")
else:
    print("❗ mp3 경로 인자 없음")
    sys.exit(1)

# === [1/4] Define Wrappers ===
print("🚀 [1/4] Defining component wrappers...")

class WhisperWrapper:
    def __init__(self, model_name="small"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - Whisper using device: {self.device}")
        self.model = whisper.load_model(model_name, device=self.device)
    def transcribe(self, audio_path):
        if not os.path.exists(audio_path): return None
        return self.model.transcribe(audio_path, language="ko")["text"]

class ElectraForTokenClassification(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.electra = ElectraModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.electra.config.hidden_size, num_labels)
    def forward(self, input_ids, attention_mask=None):
        return {"logits": self.classifier(self.electra(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state)}

class MedicalNerWrapper:
    def __init__(self, model_weights_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - NER model using device: {self.device}")
        self.label_list = ["O", "B-Disease", "I-Disease", "B-Treatment", "I-Treatment", "B-Body", "I-Body"]
        self.id2label = {i: label for i, label in enumerate(self.label_list)}
        base_model_name = "SungJoo/medical-ner-koelectra"
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.model = ElectraForTokenClassification(base_model_name, len(self.label_list))
        self.model.load_state_dict(torch.load(model_weights_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        self.mecab = Mecab(dicpath="/opt/homebrew/lib/mecab/dic/mecab-ko-dic")
    def _reconstruct_terms(self, tokens, labels):
        terms, current_term = [], ""
        for token, label in zip(tokens, labels):
            if label.startswith("B-"):
                if current_term: terms.append(current_term)
                current_term = token
            elif label.startswith("I-"): current_term += token
            elif current_term:
                terms.append(current_term)
                current_term = ""
        if current_term: terms.append(current_term)
        return terms
    def extract_terms(self, text):
        tokens = self.mecab.morphs(text)
        tokenized_inputs = self.tokenizer(tokens, truncation=True, is_split_into_words=True, return_tensors="pt")
        inputs_for_model = {k: v.to(self.device) for k, v in tokenized_inputs.items() if k != "token_type_ids"}
        with torch.no_grad():
            logits = self.model(**inputs_for_model)["logits"]
        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()
        word_ids = tokenized_inputs.word_ids(batch_index=0)
        final_labels = ["O"] * len(tokens)
        for token_idx, word_idx in enumerate(word_ids):
            if word_idx is not None and predictions[token_idx] != -100:
                final_labels[word_idx] = self.id2label[predictions[token_idx]]
        return list(dict.fromkeys(self._reconstruct_terms(tokens, final_labels)))

class QwenWrapper:
    def __init__(self, model_path="Qwen/Qwen2.5-3B"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - QWEN model using device: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)
        self.generator = TextGenerationPipeline(model=self.model, tokenizer=self.tokenizer, device=0 if self.device == "cuda" else -1)

    def explain_terms(self, term_list):
        explanations = {}

        # 🔍 디버깅 로그
        print(f"[🧪 DEBUG] 입력된 term_list: {term_list}")
        print(f"[🧪 DEBUG] term_list 타입: {type(term_list)}")

        if not term_list or not isinstance(term_list, list):
            print("ℹ️ 용어 리스트가 비어 있거나 잘못된 타입입니다. Qwen 실행 생략.")
            return explanations

        for term in term_list:
            prompt = f"의학 용어 '{term}'에 대해서 초등학생도 이해할 수 있도록 쉽게 설명해줘."
            print(f"[🧪 DEBUG] prompt: {prompt}")
            try:
                output = self.generator(prompt, max_new_tokens=256)
                print(f"[🧪 DEBUG] output: {output}")
                explanations[term] = output[0]['generated_text']
            except Exception as e:
                print(f"[❌ EXCEPTION] Qwen 생성 실패: {e}")
                raise e

        return explanations



print("✅ Component wrappers defined.")

# === [2/4] Define Main Pipeline ===
print("\n🚀 [2/4] Defining main pipeline with sequential loading...")

class MedicalVoicePipeline:
    def __init__(self, whisper_config, ner_config, qwen_config):
        self.whisper_config = whisper_config
        self.ner_config = ner_config
        self.qwen_config = qwen_config

    def run(self, audio_path):
        print("\n" + "="*50 + "\n🔬 PIPELINE RUNNING 🔬\n" + "="*50)

        print("\n[Step 1/3] Transcribing...")
        whisper_model = WhisperWrapper(**self.whisper_config)
        text = whisper_model.transcribe(audio_path)
        del whisper_model
        gc.collect()
        torch.cuda.empty_cache()
        print("✅ Whisper done.")
        if not text:
            print("🛑 Transcription failed.")
            return "", {}

        print(f"✅ Transcription:\n{text}\n")

        print("[Step 2/3] NER extracting...")
        ner_model = MedicalNerWrapper(**self.ner_config)
        terms = ner_model.extract_terms(text)

        print(f"[DEBUG] 추출된 terms: {terms}")
        if not terms or not isinstance(terms, list):
            print("🛑 NER 결과가 없거나 리스트 아님 → Qwen 실행 생략")
            return text, {}

        del ner_model
        gc.collect()
        torch.cuda.empty_cache()
        print(f"✅ Terms: {terms}")

        print("[Step 3/3] Explaining terms...")
        print(f"[DEBUG] Qwen에 전달될 terms: {terms}")
        print(f"[DEBUG] type: {type(terms)} / len: {len(terms)}")

        qwen_model = QwenWrapper(**self.qwen_config)

        
        explanations = qwen_model.explain_terms(terms)
        del qwen_model
        gc.collect()
        torch.cuda.empty_cache()
        print("✅ Explanations done.")

        return text, explanations

print("✅ Main pipeline defined.")

# === [3/4] Check model paths ===
print("\n🚀 [3/4] Checking for model files...")

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ner_path = os.path.join(BASE_DIR, "model", "ner_saved_model", "model_weights.pth")
qwen_path = os.path.join(BASE_DIR, "model", "qwen_model")

if not os.path.exists(ner_path):
    print(f"❌ NER model not found at: {ner_path}")
    sys.exit(1)
if not os.path.exists(qwen_path):
    print(f"❌ Qwen model not found at: {qwen_path}")
    sys.exit(1)

print("✅ Model files found.")

# === [4/4] Convert MP3 and run pipeline ===
print("\n🚀 [4/4] Starting execution...")

def convert_mp3_to_wav(mp3_path, wav_path="converted_audio.wav"):
    if not os.path.exists(mp3_path):
        raise FileNotFoundError(f"입력 파일이 없습니다: {mp3_path}")
    result = subprocess.run(["ffmpeg", "-y", "-i", mp3_path, wav_path],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        print(result.stderr.decode())
        raise RuntimeError("❌ MP3 → WAV 변환 실패")
    print(f"✅ MP3 → WAV 변환 완료: {wav_path}")
    return wav_path

try:
    wav_path = convert_mp3_to_wav(mp3_path)

    whisper_config = {'model_name': 'small'}
    ner_config = {'model_weights_path': ner_path}
    qwen_config = {'model_path': qwen_path}

    pipeline_instance = MedicalVoicePipeline(whisper_config, ner_config, qwen_config)
    text, explanations = pipeline_instance.run(wav_path)

    # 저장
    json_save_path = mp3_path.replace(".mp3", ".json")
    with open(json_save_path, "w", encoding="utf-8") as f:
        json.dump({"text": text, "explanations": explanations}, f, ensure_ascii=False, indent=2)
    print(f"✅ JSON 저장 완료: {json_save_path}")

except Exception as e:
    print(f"❌ 실행 중 오류 발생: {e}")